%%
%% This is file `lexample.tex',
%% Sample file for siam macros for use with LaTeX 2e
%%
%% October 1, 1995
%%
%% Version 1.0
%%
%% You are not allowed to change this file.
%%
%% You are allowed to distribute this file under the condition that
%% it is distributed together with all of the files in the siam macro
%% distribution. These are:
%%
%%  siamltex.cls (main LaTeX macro file for SIAM)
%%  siamltex.sty (includes siamltex.cls for compatibility mode)
%%  siam10.clo   (size option for 10pt papers)
%%  subeqn.clo   (allows equation numbners with lettered subelements)
%%  siam.bst     (bibliographic style file for BibTeX)
%%  docultex.tex (documentation file)
%%  lexample.tex (this file)
%%
%% If you receive only some of these files from someone, complain!
%%
%% You are NOT ALLOWED to distribute this file alone. You are NOT
%% ALLOWED to take money for the distribution or use of either this
%% file or a changed version, except for a nominal charge for copying
%% etc.
%% \CharacterTable
%%  {Upper-case    \A\B\C\D\E\F\G\H\I\J\K\L\M\N\O\P\Q\R\S\T\U\V\W\X\Y\Z
%%   Lower-case    \a\b\c\d\e\f\g\h\i\j\k\l\m\n\o\p\q\r\s\t\u\v\w\x\y\z
%%   Digits        \0\1\2\3\4\5\6\7\8\9
%%   Exclamation   \!     Double quote  \"     Hash (number) \#
%%   Dollar        \$     Percent       \%     Ampersand     \&
%%   Acute accent  \'     Left paren    \(     Right paren   \)
%%   Asterisk      \*     Plus          \+     Comma         \,
%%   Minus         \-     Point         \.     Solidus       \/
%%   Colon         \:     Semicolon     \;     Less than     \<
%%   Equals        \=     Greater than  \>     Question mark \?
%%   Commercial at \@     Left bracket  \[     Backslash     \\
%%   Right bracket \]     Circumflex    \^     Underscore    \_
%%   Grave accent  \`     Left brace    \{     Vertical bar  \|
%%   Right brace   \}     Tilde         \~}


\documentclass[final]{siamltex}

% definitions used by included articles, reproduced here for
% educational benefit, and to minimize alterations needed to be made
% in developing this sample file.

\newcommand{\pe}{\psi}
\def\d{\delta}
\def\ds{\displaystyle}
\def\e{{\epsilon}}
\def\eb{\bar{\eta}}
\def\enorm#1{\|#1\|_2}
\def\Fp{F^\prime}
\def\fishpack{{FISHPACK}}
\def\fortran{{FORTRAN}}
\def\gmres{{GMRES}}
\def\gmresm{{\rm GMRES($m$)}}
\def\Kc{{\cal K}}
\def\norm#1{\|#1\|}
\def\wb{{\bar w}}
\def\zb{{\bar z}}

% some definitions of bold math italics to make typing easier.
% They are used in the corollary.

\def\bfE{\mbox{\boldmath$E$}}
\def\bfG{\mbox{\boldmath$G$}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Actual stuff starts here


%% encoding
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}



%% Math packages
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{mathrsfs}

\usepackage{color}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{listings}


\newcommand{\mycomment}[1]{{\color{blue} #1}}

\title{Abstract Theories}

% The thanks line in the title should be filled in if there is
% any support acknowledgement for the overall work to be included
% This \thanks is also used for the received by date info, but
% authors are not expected to provide this.

\author{Zenna Tavares\thanks{Massachusetts Institute of Technology}
}
\begin{document}

\maketitle

\begin{abstract}
Theories and programs are expressive objects.
It expressiveness 


\end{abstract}

\begin{keywords}
\end{keywords}

% s
Humans possess, and consistently execute a remarkable capacity for constructing theories of structural complexity.
Physical theories such as the law of gravitation invole iteration over variable quantities of objects, psychological theories of mind and game playing may involve recursively considering the intentions of agents, and generative theories of vision and physics exploit on simulation.
The composition of the kind of theories developed both intuitively and delibaratively, and formally and informally.
This analogue has led to the hypothesis that sufficiently rich representation for theories is something like, or explicitly is a program.

RECAP ON JBT STUFF

Computational level theory - What's that all about

\section*{Theories and Programs}
A strict interpretation of computationalism projects beyond the metaphorical; the mind is not merely explained by computational processes, nor just is it amenable to computer simulation, but {\em is} computational.
Cognition, it is argued, is as much the execution of programs as is its instruction based, silicon embedded counterparts.
The plausibility of this claim leans heavily on the idea of a universal machine, and evidenced by the sheer.
Second only in surprisingness of the idea that there could even exist a universal computer, is the ease , it doesn't take much, and the diversity of models of computation stands testtony to this.
A complete theory demands more however, if the mind is a computer, then what kind of computer?
The question addressed by this study takes takes the last of the aforementioned examples and aims to concretise an emergising idea that theories explanations are programs.
What is the nature of the relationship of the relation between theories and programs, what kind of program is a theory?

But what is a theory?
Functionally, theories impose constraints on possible worlds; they declare what is possible, or probable conditioned on evidence.
Variety expands across numerous dimensions: the extent to which latent variables are appealed to, whether or not relationships evoke a causal nature, the rigour through which they have been evaluated, and the formality with which they are stated or communicated.
Language has partitioned these variations correspondingly, with a terms such as hypothesis, explanation , yet little consensus on the extent to which these are the same, or what precisely differentiates them.

The argument here is that there is a correspondance between kinds of theory and kinds of programs.
Declarative theories as declarative programs.

The fact that ideas should logically entail one another is nothing but a fact, no more understandable by itself than any fact of the material world.
A logical expression is the expression of natural phenomunom , as ithe vravitation or the growth of a tree.
The fact that classical loogic came short of modelling man characteristics of human reasoning, such as deficiencies of materia pimplication, non-monotony, limited resources, failure to take itime into accout, etc. led from a theoretical and pracitcalpoin to many important deveopments, among which is neo classical logics.
One should finally notice that logic, as is the case for all topoics in science changes with time in its undaemental principels, which can be interpreted different tly as time passes by, in its methods notation, topics of interest etc.
Why so many logics?  A given logic can be chosen because itis better adapted than another one, for example because it has more expressive power (we can say mor ethings)
 =, or becaue it allows us to say things more naturally, easier to automate, etc.

T provide an existence proof that is not constructive is like announcing that ther is a treasure hidden somewhere, without sayng where it is exactly.

A logic is a collection of mathematical structures, a collection of formal expressions and a satifactor relation between these two collection.

Intuitive models and generative models
Predictive theories as program executions, generative models conditioned on some kind of evidence.
Probabilstic programs are emerging as a particularly expressive representation for expressing generative models.
A program is a theory, and a computation is a deduction from that theory.

\section*{The cause of failure}
The syntactic form of a program is indicative of its semantics only to an intelligent reasoner; the mapping is complex.
This, and the inherent discreteness of logical structures contributes to what is refered tp discontinuity.
It should be made explicit though, that any such notion assumes a metric on the space of programs, presumably evaluated syntatically.
Together these have prohibit the use of simple based which perform well in other domains
The suggested alternatives have been to transform the search space, such that a natural metric without the adverse properties can be defined, to smooth the landscape.

\section*{Constructive Bias}
Bias is necessary for induction, machine learning and search.
All learning algorithms employ some mechanism to restrict the hypothesis space, or prefer one hypothesis over another.
These mechanisms are collectively known as inductive bias.
It is illuminating to dissect from a bias its declarative component, which describes the manner in which the hypothesis space is being weighted.
In this declarative sense, bias is equivalent to a number of terms, in particular constraints.
It is in the other properties of bias, which cause these terms to differ, and are presumably the reaso we have different words for them.
This exemplifies one axes of variation of bias, that of constructiveness versus restrictiveness, which I belive lies at the heart of the.

What determines the extend to which a particular constraint can be used onstructively?
A case against local methods


% Halley’s discovery is an example of causal induction: inferring
% causal structure from data.

% Choosing between these
% hypotheses required the use of statistical inference.

To the extent that the prior knowledge is veridical—
when people’s abstract intuitive theories reflect the way causal
systems in their environment tend to work— our rational framework explains how people’s inferences about the structure of
specific causal systems can be correct, even given very little data

, where the
strength of the constraints provided by prior knowledge gradually
increases, and the amount of information required in order to make
a causal inference decreases accordingly

Theories, it appears.
Formal scientific theories, everyday theories known as intuitive or naive theories, explanation.
There is little consensus.

\section{Discovering Declarative Structure in Data}

Theories are often formally expressed as equalities and inequalities: logical assertions on the equivalence of two expressions.
An expression is a finite combination of symbols belonging to some formal language.
The representational power of this language, and probable cause of its permanance and pervaseivness throughout modern science,  derives primarily from two sources: its use of syntax and symbolic form, the its of computational constructs.
% a from, notions of compositionality, systematicity and productivity, in expressing theories.
% Of particular importance to mathemticians is conciseness, indeed new notation is often used to avoid cumbersome verbose code.

Given a dataset of variables $\mathcal{D} = (x_1, x_2, ..., x_n)$, each composed of $N$ corresponding real-valued datapoints $x_i \in \mathbb{R}^N$, we aim to discover a system of equalities and inequalities which govern the data.
We represent equalities as {\em s-expressions}, a notation for nested lists, popularised by dialects of the programming language Lisp.
No hard constraints on the class of models we are willing to consider are imposed; search is not constrained only to say polynomials, linear equations or even functional relationships.

For an equation to govern the data in this sense is subtle.
We must relax the hardness of logical constraints in order to deal with real world data corrupted with noise.
We must also introduce some inductive bias to prefer some equalities over the infinite number of alternatives.
Lastly we must provide some means to avoid tautologies and identities; equalities which are evidently true and may even be supported on the data, but provide no constraints.

Our objectives can be stated formally in a probabilstic framework as an inductive inference problem , we seek to maximise the posterior probability $P(e \vert D)$ where $e \in \mathcal{E}$ is an expression in a subset of lisp given by the following grammar.

EXPRESSIONG GRAMMAR HERE

The discovery of equations on the reals is often cast as as ymbolic regression problem.
The most prominent approach is to cast the problem as a global optimisation problem, and use heuristic hearhc methods to enact syntax manipulations in the hope there will be some form of a error gradient that can be followed.

There are two main ideas.  First, search should proceed by abstract constraints, these constraints should be (partially at least) extracted from the data.
Second is that error.

\section*{Model Invariants}

Hypotheses constructed is directed by data.
We are able to infer constraints on valid proposals by first noticing regularities and structure in data.
Given data drawn from the functional relationship $y=sinx+x^2$.
Evaluating attributes may be automatic or deliberative, procedural or parallel, and the responsibility of which is distributed among lower level sensory areas.
We however can consider these uniformly as programs to be evaluated on the data, and for simplicity we constrain these to be predicates on the data.

The framework, as stated later in the algorithm section, proceeds genrally by extending models.
A model is an equation composed of variables, parameters and functions.
Each parameter has an associated generative model, which is a stochastic function composed of determinsitic and stochastic primitives.
For illustration a power model $y=ax^n$ with normally distributed parameters is expressed as:

\begin{verbatim}

(def power-model
  {:as-expr '(= y (Math/pow x n)) 
  :params {'a (randn 1 1) 'n (randn 1 1)}
  :vars ['y 'x]
  :name 'power})

\end{verbatim}

An primitive set of models is supplies $M'$

Our first step is to cosntruct a first good guess.
This is achived by evaluating a set of attributes which are predicates deigned to capture abstract features of the data.
Much in line with the theme of this work, an attribute is a progre data is composedam, composed explicitly of a declarative and procedural component.

Attributes are evaluated against a dataset to produce an attribute vector $(a_1,...,a_n)$ where $a_i \in prop$.
Then as a first approximation towards computing $P(e | D)$, we seek to compute $P(m | a)$.

Our approach to this problem is generative
The simple example above demonstrates a hurdle,
The approach suggested here is to exploit the fact that our expressions can be evaluable by generating extensions to our model

\subsubsection{Pattern grammar}

Formally a grammar is a  $(P, f_P  N, \Sigma, R, \theta, ) $
\begin{itemize}
\item A finite set $P$ of patterns.
\item A pattern matcher
\item A probability mass function $f_P$ on pattern matches.
\item A finite set $N$ of nonterminal symbols.
\item A finite set $\Sigma$ of terminal symbols that is disjoint from N
\item A finite set $R$ of parsing rules.
\item A correspondng set of probabilities on rules such that
\end{itemize}

Unlike a context free grammar or stochastic context free grammar, a pattern grammar neither recognises strings nor generates new strings; it extends existing strings.
To apply a modification grammar to a sentence means to apply perform all pattern matches on the string, sample one from some distribution.

Patterns Grammars

\section*{Difference reduction}

A common technique employed in the sciences and regression analysis is to inspect the residuals, or fitting error.

Considering a variable $x$, we call the additive error with respect to model $m \in \mathcal{M}$, the dataset $R_+ = (r_i \vert r_i = y_i - m(x_i))$.  Note that least squares regression seeks to minimise $S = \sum_{i=1}^n r_i^2$ . We would like to generalise this in two ways, firstly to account for n-ary conditions, and secondly to generalise error from additive to a general function.

One central procedure is to establish a template form, and explore ways in which this form
would be modified in order to account for its deficiencies.
Consider a set of characteristic features $C$, each a predicate on the data, $f \in C:\mathbb{R}^n \rightarrow {true, false}$.
Examples might be: periodic? smoothly varying? constant-period?.
We then define a conditional probability table for each model, $P(model \vert C)$.
The first objective then is to learn this probability table.
We then ask hypotheticals, perturbations to our conditioned attributes to create modified $C'$, and observe the affect on p(model|C).
For perturbations which increase the $p(model \vert C)$, e.g. if the data were smoothly varying then a sinusoid becomes a more likely place to start, so then, what do we have to do to change out data to make it smoothly varying, or what do we have to do to our model to make it not smoothly varying.
What change do we need to make to our model, such that this characteristic is no longer. 

Formally the task is to

Define or learn a set of predicates $C$.
For each model $m$, learn a conditional probability table $p(m|C)$
Considering $C$ and perturbed function $C'$, let $C \rightarrow C'$ as the set   Learn the perturbation function

\section*{Algorithm}

At thie highest level, the algorithm constructs equations by executing transformations on sets of equations; it can be viewed as traversing the powerset of equations.
Transformations are data driven and are based on t

Recognition (or proxy thereof):, the idea that abstract.
Extension:
First, the idea that abstract.

In words, we attempt to recognise some general structure in the data.  Currently this is implemented by iterating over the set of models.
Then we attempt to extend a candidate model, by first proposing an extension



\bibliographystyle{siam}
\bibliography{library}

\end{document}